

import numpy as np
import matplotlib.pyplot as plt

# Define a range of inputs for visualization
x = np.linspace(-10, 10, 100)

# 1. Sigmoid Activation Function
# Formula: f(x) = 1 / (1 + exp(-x))
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# 2. ReLU (Rectified Linear Unit)
# Formula: f(x) = max(0, x)
def relu(x):
    return np.maximum(0, x)

# 3. Softmax Activation Function
# Formula: f(x_i) = exp(x_i) / sum(exp(x_j))
def softmax(x):
    e_x = np.exp(x - np.max(x)) # Subtracting max for numerical stability
    return e_x / e_x.sum()

# 4. Tanh (Hyperbolic Tangent)
# Formula: f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))
def tanh(x):
    return np.tanh(x)

# 5. Leaky ReLU
# Formula: f(x) = x if x > 0 else alpha * x (where alpha is small, e.g., 0.01)
def leaky_relu(x, alpha=0.1):
    return np.where(x > 0, x, x * alpha)

# 6. ELU (Exponential Linear Unit)
# Formula: f(x) = x if x > 0 else alpha * (exp(x) - 1)
def elu(x, alpha=1.0):
    return np.where(x > 0, x, alpha * (np.exp(x) - 1))

# 7. Binary Step
# Formula: f(x) = 1 if x >= 0 else 0
def binary_step(x):
    return np.where(x >= 0, 1, 0)
